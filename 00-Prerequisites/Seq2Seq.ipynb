{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b374fd47-efdb-4a51-baf8-5c159af6cafb",
   "metadata": {},
   "source": [
    "# Encoding, Decoding, and Learning: The Math of Sequence-to-Sequence Translation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b87ada3-4a51-4815-b7f1-fd8026e0868f",
   "metadata": {},
   "source": [
    "In this post, you'll explore the **mathematical foundations of sequence-to-sequence (seq2seq) models** for machine translation. We'll focus on the **RNN Encoder‚ÄìDecoder architecture**, which consists of two main components: an **encoder RNN** and a **decoder RNN**. As an example, we'll see how this model can be trained to translate English phrases into French üá¨üáß‚û°Ô∏èüá´üá∑.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe133a4-9e8a-4f29-b5fa-f6618981c051",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Definition</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13784f71-edd5-409c-8139-4748c25d121e",
   "metadata": {},
   "source": [
    "#### $\\textcolor{green}{\\text{Sequence-to-Sequence (Seq2Seq) Translation }}$\n",
    "\n",
    "Sequence-to-Sequence (Seq2Seq) is a deep learning framework designed to convert an input sequence (e.g., a sentence in French) into an output sequence (e.g., its English translation). It consists of two main components:\n",
    "\n",
    "- **$\\textcolor{green}{\\text{ Encoder}}$** ‚Äì Processes the input sequence and compresses it into a $\\textcolor{green}{\\text{context vector}}$, a fixed-length representation capturing the input's semantics.\n",
    "\n",
    "- **$\\textcolor{green}{\\text{ Decoder}}$** ‚Äì Generates the output sequence $\\textcolor{green}{\\text{step-by-step}}$, conditioned on the context vector from the encoder.\n",
    "\n",
    "\n",
    "\n",
    "**Key Features**\n",
    "\n",
    "- **Handles variable-length input and output** sequences (e.g., translating sentences of different lengths).\n",
    "- **Supports various architectures**: RNNs, LSTMs, GRUs, and more recently, **Transformers** (state-of-the-art).\n",
    "- **Trained end-to-end** using *teacher forcing*  i.e., during training, the decoder is fed the true previous token rather than its own prediction.\n",
    "\n",
    "\n",
    "$\\textcolor{green}{\\text{ }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cd00a8-f0d9-44f8-9203-017a9dc26386",
   "metadata": {},
   "source": [
    "  <h1 align=\"center\">Preliminaries: Recurrent Neural Networks</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adde301-40d4-45e9-87cd-f51fc1b61ad7",
   "metadata": {},
   "source": [
    "<!-- :::{tip}  Recurrent Neural Networks -->\n",
    "\n",
    "![](../images/RNN.png)\n",
    "**Figure 1**: RNN Architecture (Image by the author).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### The input $\\textcolor{green}{x_t}$\n",
    "In a Recurrent Neural Network (RNN), the input at time step $t$, denoted as $\\textcolor{green}{x_t}$, represents the data fed into the network at that point in the sequence. It is typically a vector (e.g., a word embedding) in $\\mathbb{R}^d$, where $d$ is the embedding dimension.\n",
    "\n",
    "\n",
    "As an example, lte's consider the 4-word sentence: $ \\textcolor{green}{\\text{``Cryptocurrency is the future''}}$. This sequence has length $m = 4$. The corresponding inputs are:\n",
    "\n",
    "$$\n",
    "\\textcolor{green}{x_1} = \\text{embedding}(\\textcolor{green}{\\text{``Cryptocurrency''}});\\quad  \n",
    "\\textcolor{green}{x_2} = \\text{embedding}(\\textcolor{green}{\\text{``is''}}) ; \\quad  \n",
    "\\textcolor{green}{x_3} = \\text{embedding}(\\textcolor{green}{\\text{``the''}}); \\quad   \n",
    "\\textcolor{green}{x_4} = \\text{embedding}(\\textcolor{green}{\\text{``future''}})\n",
    "$$\n",
    "\n",
    "### Hidden State $\\textcolor{red}{h_t}$\n",
    "\n",
    "The hidden state serves as the network's internal memory, encoding contextual information from the sequence up to time $t$. It is updated as shown above, and plays a key role in maintaining temporal dependencies. At each time step, the hidden state $\\textcolor{red}{h_t}$ is updated based on the current input $\\textcolor{green}{x_t}$ and the previous hidden state $\\textcolor{red}{h_{t-1}}$:\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{h_t} = f(W_{hh} \\textcolor{red}{h_{t-1}} + W_{xh} \\textcolor{green}{x_t} + b_h)\n",
    "$$\n",
    "\n",
    "Here $W_{xh}$ and $W_{hh}$ are weight matrices, $b_h$ is a bias vector, and $f$ is a nonlinear activation function such as $\\tanh$ or ReLU.\n",
    "\n",
    "### Initial Hidden State $\\textcolor{red}{h_0}$\n",
    "\n",
    "The initial hidden state $\\textcolor{red}{h_0}$ represents the starting memory of the RNN before any input is processed. It is typically initialized in one of the following ways:\n",
    "\n",
    "- As a zero vector:  \n",
    "  $\n",
    "  \\textcolor{red}{h_0} = \\mathbf{0} \\in \\mathbb{R}^n\n",
    "  $\n",
    "  where $n$ is the dimensionality of the hidden state.\n",
    "\n",
    "- With small random values (e.g., sampled from a normal distribution):  \n",
    "  $\n",
    "  \\textcolor{red}{h_0} \\sim \\mathcal{N}(0, \\sigma^2 I)\n",
    "  $\n",
    "\n",
    "- As a learned parameter:  \n",
    "  $\\textcolor{red}{h_0}$ can also be treated as a trainable vector that is learned during training, allowing the model to adapt its initial memory based on the data.\n",
    "\n",
    "The choice depends on the specific task, model design, and desired behavior at the start of the sequence.\n",
    "\n",
    "\n",
    "### Output $\\textcolor{blue}{y_t}$\n",
    "\n",
    "The output at time step $t$ is computed from the hidden state:\n",
    "\n",
    "$$\n",
    "\\textcolor{blue}{y_t} = g(W_{hy} \\textcolor{red}{h_t} + b_y)\n",
    "$$\n",
    "\n",
    "where $W_{hy}$ is the output weight matrix, $b_y$ is a bias vector and $g$ is an activation function such as softmax or sigmoid, depending on the task.\n",
    "\n",
    "\n",
    "\n",
    "<!-- ::: -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "133352bd-7e20-4196-a0e0-110794754ba0",
   "metadata": {},
   "source": [
    ":::{tip}  Sequence Modeling vs. Sequence Prediction\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- **Sequence modeling** refers to learning the joint probability distribution of a sequence:\n",
    "  \n",
    "  $$\n",
    "  p(\\textcolor{green}{x_1}, \\textcolor{green}{x_2}, \\dots, \\textcolor{green}{x_T}) = \\prod_{t=1}^T p(\\textcolor{green}{x_t} \\mid \\textcolor{green}{x_1}, \\dots, \\textcolor{green}{x_{t-1}})\n",
    "  $$\n",
    "\n",
    "  In this context, the model learns to estimate each conditional distribution $p(\\textcolor{green}{x_t} \\mid  \\textcolor{green}{x_{<t}} )$. The output $\\textcolor{blue}{y_t}$ is interpreted as:\n",
    "\n",
    "  $$\n",
    "  \\textcolor{blue}{y_t} \\approx p(\\textcolor{green}{x_t} \\mid  \\textcolor{green}{x_{<t}} )\n",
    "  $$\n",
    "\n",
    "  That is, $\\textcolor{blue}{y_t}$ is a **model-generated approximation** of the next-token distribution, based on past inputs.\n",
    "\n",
    "- **Sequence prediction**, on the other hand, is a downstream task that uses the output $\\textcolor{blue}{y_t}$ to **predict** the next token:\n",
    "\n",
    "  $$\n",
    "  \\textcolor{green}{\\hat{x}_t} = \\arg\\max_{\\textcolor{blue}{j}} \\textcolor{blue}{y_{t,j}} \n",
    "  $$\n",
    "\n",
    "  Here, $\\textcolor{blue}{y_t}$ is used to choose the most likely next symbol (i.e., prediction), but it still represents a distribution over the vocabulary.\n",
    "\n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70747296-5016-4762-a2d1-242bdf1c7700",
   "metadata": {},
   "source": [
    " <h1 align=\"center\">RNN Encoder‚ÄìDecoder</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4303879d-c83e-4bf2-8856-8c9780b34dc0",
   "metadata": {},
   "source": [
    "The RNN Encoder‚ÄìDecoder architecture, introduced by [](doi:10.3115/v1/D14-1179) and [](doi:10.48550/arXiv.1409.3215), operates by encoding an input sentence into a fixed-length vector and then decoding it to generate an output sequence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "![](../images/Encode-Decoder.png)\n",
    "\n",
    "**Figure 2**: Encode-Decoder architecture (Image and comment from [](doi:10.3115/v1/D14-1179)).\n",
    "\n",
    "- In this framework, the encoder processes the input sequence ${\\textbf{x}} = (\\textcolor{green}{x_1}, \\textcolor{green}{x_2}, \\dots, \\textcolor{green}{x_T} )$ and transforms it into a context vector $\\textcolor{red}{c}$. Typically, a recurrent neural network (RNN) is employed for this transformation as follows:\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{h_t} = f( \\textcolor{red}{h_{t-1}} , \\textcolor{green}{x_t})\n",
    "$$\n",
    "\n",
    "and\n",
    "\n",
    "$$\n",
    "\\textcolor{red}{c}= q(\\{ \\textcolor{red}{h_1}, \\dots , \\textcolor{red}{h_T} \\})\n",
    "$$\n",
    "\n",
    "Here, $\\textcolor{red}{h_t} \\in \\mathbb{R}^n$ represents the hidden state at time step $t$, and the context vector $\\textcolor{red}{c}$ is derived from the sequence of hidden states. The functions $f$ and $q$ are nonlinear operations; for example, [](doi:10.48550/arXiv.1409.3215) used an LSTM for $f$ and set $q(\\{ \\textcolor{red}{h_1}, \\dots , \\textcolor{red}{h_T} \\}) = \\textcolor{red}{h_T}$.\n",
    "\n",
    "- The decoder is trained to generate each word $\\textcolor{blue}{y_{t'}}$ based on the context vector $\\textcolor{red}{c}$ and the previously generated words $\\{\\textcolor{blue}{y_1}, \\dots , \\textcolor{blue}{y_{t'-1}} \\}$. It models the probability distribution over the output sequence $\\textbf{y}$ by factorizing it into a product of conditional probabilities:\n",
    "\n",
    "$$\n",
    "p(\\textbf{y}) = \\prod_{t'=1}^{T'} p(\\textcolor{blue}{y_{t'}} | \\{\\textcolor{blue}{y_1}, \\dots , \\textcolor{blue}{y_{t'-1}} \\}, \\textcolor{red}{c})\n",
    "$$\n",
    "\n",
    "where $\\textbf{y} = (\\textcolor{blue}{y_1}, \\dots , \\textcolor{blue}{ y_{T'}} )$.\n",
    "\n",
    "When using an RNN, each conditional probability is computed as $p(\\textcolor{blue}{y_{t'}} | \\{\\textcolor{blue}{y_1}, \\dots , \\textcolor{blue}{y_{t'-1}} \\}, \\textcolor{red}{c}) = g(\\textcolor{blue}{y_{t'-1}}, \\textcolor{red}{h_{t'}}, \\textcolor{red}{c})$, where $g$ is a nonlinear function (possibly multi-layered) that outputs the probability of $\\textcolor{blue}{y_{t'}}$, and $\\textcolor{red}{h_{t'}}$ denotes the decoder‚Äôs hidden state at time $t'$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb35667-67d5-48a4-ba46-4672ead55499",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Time to Dive into the Implementation üíª</h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daeb3501-02ba-4c8c-a816-2c10ba9fc99e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
